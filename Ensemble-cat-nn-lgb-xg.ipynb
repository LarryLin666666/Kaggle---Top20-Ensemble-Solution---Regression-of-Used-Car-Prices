{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":76728,"databundleVersionId":9057646,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport gc\nimport numpy as np\nimport pandas as pd\n\nimport optuna\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nfrom catboost import CatBoostRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, Input, Embedding, Concatenate, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import Lasso\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Reduce memory usage by optimizing data types\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != object and col_type.name != 'category':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type).startswith('int'):\n                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                else:\n                    df[col] = df[col].astype(np.int32)\n            else:\n                df[col] = df[col].astype(np.float16)\n    end_mem = df.memory_usage().sum() / 1024**2\n    return df\n\n# Load the data\ntrain = pd.read_csv('/kaggle/input/playground-series-s4e9/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s4e9/test.csv')\n\n# # Define the sampling fraction\n# sample_frac = 0.0001  # 1%\n\n# # Sample 1% of the training and test data\n# train = train.sample(frac=sample_frac, random_state=42)\n# test = test.sample(frac=sample_frac, random_state=42)\n\n# # Save the sampled datasets\n# train.to_csv('train_sampled.csv', index=False)\n# test.to_csv('test_sampled.csv', index=False)\n\n# print(f\"Sampled Training Data Shape: {train.shape}\")\n# print(f\"Sampled Test Data Shape: {test.shape}\")\n\nprint(f\"Training Data Shape: {train.shape}\")\nprint(f\"Test Data Shape: {test.shape}\")\nprint(train.head())\nprint(test.head())\n# Clear memory\ngc.collect()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-23T23:19:11.410477Z","iopub.execute_input":"2024-09-23T23:19:11.410973Z","iopub.status.idle":"2024-09-23T23:19:32.949880Z","shell.execute_reply.started":"2024-09-23T23:19:11.410924Z","shell.execute_reply":"2024-09-23T23:19:32.948622Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill missing values\n\ntrain['clean_title'].fillna('No', inplace=True)\ntrain['accident'].fillna('None reported', inplace=True)\n\ntest['clean_title'].fillna('No', inplace=True)\ntest['accident'].fillna('None reported', inplace=True)\n\nfuel_type_mapping = {\n    '1500 Laramie': 'Gasoline',  # Typically powered by V6 or V8 engines\n    '1500 TRX': 'Gasoline',  # Powered by a 6.2L supercharged V8 engine\n    '500e Battery Electric': 'Electric',  # Fully electric\n    'A4 2.0T Premium': 'Gasoline',  # 2.0L turbocharged inline-four engine\n    'A4 2.0T Tech Premium': 'Gasoline',  # 2.0L turbocharged inline-four engine\n    'A5 2.0T Premium Plus': 'Gasoline',  # 2.0L turbocharged inline-four engine\n    'A7 55 Premium Plus': 'Gasoline',  # 3.0L V6 engine\n    'Air Grand Touring': 'Electric',  # Fully electric luxury sedan\n    'Air Pure': 'Electric',  # Fully electric luxury sedan\n    'AMG G 63 Base': 'Gasoline',  # 4.0L twin-turbo V8 engine\n    'AMG GLE AMG GLE 63 S-Model 4MATIC': 'Gasoline',  # 4.0L twin-turbo V8 engine\n    'AMG GLS 63 4MATIC': 'Gasoline',  # 4.0L twin-turbo V8 engine\n    'Armada Platinum': 'Gasoline',  # 5.6L V8 engine\n    'Armada SL': 'Gasoline',  # 5.6L V8 engine\n    'Bentayga Activity Edition': 'Gasoline',  # V8 engine\n    'Bolt EUV Premier': 'Electric',  # Fully electric\n    'Bolt EV LT': 'Electric',  # Fully electric\n    'Bronco': 'Gasoline',  # Varies by trim, typically a V6 engine\n    'Bronco Outer Banks': 'Gasoline',  # 2.3L turbocharged I4 engine\n    'Bronco Sport Big Bend': 'Gasoline',  # 1.5L turbocharged I3 engine\n    'Bronco Wildtrak Advanced': 'Gasoline',  # 2.7L twin-turbo V6\n    'bZ4X Limited': 'Electric',  # Fully electric\n    'C40 Recharge Pure Electric Twin Ultimate': 'Electric',  # Fully electric\n    'CC Sport': 'Gasoline',  # 2.0L turbocharged I4 engine\n    'Challenger SRT Demon': 'Gasoline',  # 6.2L supercharged V8 engine\n    'Charger GT': 'Gasoline',  # 3.6L V6 engine\n    'Cooper S Base': 'Gasoline',  # 2.0L turbocharged I4 engine\n    'Corvette Stingray w/2LT': 'Gasoline',  # 6.2L V8 engine\n    'CT5-V Blackwing': 'Gasoline',  # 6.2L supercharged V8 engine\n    'E-Class E 400 4MATIC': 'Gasoline',  # 3.0L V6 engine\n    'e-Golf SE': 'Electric',  # Fully electric\n    'EQS 450 4MATIC': 'Electric',  # Fully electric\n    'EQS 450+ Base': 'Electric',  # Fully electric\n    'Escalade Sport Platinum': 'Gasoline',  # 6.2L V8 engine\n    'e-tron Premium': 'Electric',  # Fully electric\n    'e-tron Prestige': 'Electric',  # Fully electric\n    'EV6 GT-Line': 'Electric',  # Fully electric\n    'EV6 Wind': 'Electric',  # Fully electric\n    'Evora 400 Base': 'Gasoline',  # 3.5L V6 engine\n    'Expedition Timberline': 'Gasoline',  # 3.5L V6 engine\n    'F12berlinetta Base': 'Gasoline',  # 6.3L V12 engine\n    'F-150 Lariat': 'Gasoline',  # V6 or V8 engines\n    'F-150 Lightning LARIAT': 'Electric',  # Fully electric\n    'F-150 Lightning XLT': 'Electric',  # Fully electric\n    'F-150 SVT Raptor': 'Gasoline',  # 3.5L twin-turbo V6\n    'F-150 XLT': 'Gasoline',  # V6 or V8 engines\n    'F-250 King Ranch': 'Diesel',  # 6.7L Power Stroke turbo diesel\n    'Flying Spur V8': 'Gasoline',  # 4.0L twin-turbo V8 engine\n    'Forte LXS': 'Gasoline',  # 2.0L inline-four engine\n    'ForTwo Pure': 'Electric',  # Fully electric\n    'F-PACE S': 'Gasoline',  # 3.0L supercharged V6 engine\n    'F-TYPE R': 'Gasoline',  # 5.0L supercharged V8 engine\n    'G8 GT': 'Gasoline',  # 6.0L V8 engine\n    'G90 3.3T Premium': 'Gasoline',  # 3.3L twin-turbo V6 engine\n    'Gallardo LP550-2': 'Gasoline',  # 5.2L V10 engine\n    'Gladiator Mojave': 'Gasoline',  # 3.6L V6 engine\n    'GLS 450 Base 4MATIC': 'Gasoline',  # 3.0L V6 engine\n    'Grecale Modena': 'Gasoline',  # 3.0L twin-turbo V6 engine\n    'GT-R Premium': 'Gasoline',  # 3.8L twin-turbo V6 engine\n    'GV70 3.5T Sport': 'Gasoline',  # 3.5L twin-turbo V6 engine\n    'Hardtop Cooper': 'Gasoline',  # 1.5L turbocharged I3 engine\n    'HUMMER EV Edition 1': 'Electric',  # Fully electric\n    'i3 120Ah w/Range Extender': 'Hybrid',  # Plug-in hybrid\n    'i3 94 Ah': 'Electric',  # Fully electric\n    'i3 Base': 'Electric',  # Fully electric\n    'i3 Base w/Range Extender': 'Hybrid',  # Plug-in hybrid\n    'ID.4 Pro S': 'Electric',  # Fully electric\n    'ILX Technology Plus Package': 'Gasoline',  # 2.4L inline-four engine\n    'IONIQ 5 SE': 'Electric',  # Fully electric\n    'K5 GT-Line': 'Gasoline',  # 1.6L turbocharged I4 engine\n    'Kona EV SEL': 'Electric',  # Fully electric\n    'Lancer DE': 'Gasoline',  # 2.0L inline-four engine\n    'Leaf S': 'Electric',  # Fully electric\n    'Leaf SL': 'Electric',  # Fully electric\n    'Leaf SV PLUS': 'Electric',  # Fully electric\n    'LYRIQ Luxury': 'Electric',  # Fully electric\n    'M3 Competition xDrive': 'Gasoline',  # 3.0L twin-turbo I6 engine\n    'M3 CS': 'Gasoline',  # 3.0L twin-turbo I6 engine\n    'M340 i xDrive': 'Gasoline',  # 3.0L I6 engine\n    'M440 i': 'Gasoline',  # 3.0L turbo I6 engine\n    'M8 Competition': 'Gasoline',  # 4.4L twin-turbo V8 engine\n    'Macan S': 'Gasoline',  # 3.0L turbo V6 engine\n    'MDX 3.5L w/Advance & Entertainment Pkgs': 'Gasoline',  # 3.5L V6 engine\n    'MDX w/Technology Package': 'Gasoline',  # 3.5L V6 engine\n    'Mirai Base': 'Hydrogen',  # Hydrogen fuel cell\n    'Mirai Limited': 'Hydrogen',  # Hydrogen fuel cell\n    'Model 3': 'Electric',  # Fully electric\n    'Model 3 Base': 'Electric',  # Fully electric\n    'Model 3 Long Range': 'Electric',\n    'Model 3 Mid Range': 'Electric',\n    'Model 3 Performance': 'Electric',\n    'Model 3 Standard Range': 'Electric',\n    'Model 3 Standard Range Plus': 'Electric',\n    'Model S 100D': 'Electric',\n    'Model S 70D': 'Electric',\n    'Model S 75D': 'Electric',\n    'Model S 85': 'Electric',\n    'Model S 85D': 'Electric',\n    'Model S 90D': 'Electric',\n    'Model S Long Range': 'Electric',\n    'Model S Long Range Plus': 'Electric',\n    'Model S P100D': 'Electric',\n    'Model S Performance': 'Electric',\n    'Model S Plaid': 'Electric',\n    'Model X 100D': 'Electric',\n    'Model X 75D': 'Electric',\n    'Model X Base': 'Electric',\n    'Model X Long Range': 'Electric',\n    'Model X Long Range Plus': 'Electric',\n    'Model X P100D': 'Electric',\n    'Model X P90D': 'Electric',\n    'Model X Performance': 'Electric',\n    'Model X Plaid': 'Electric',\n    'Model Y Long Range': 'Electric',\n    'Model Y Performance': 'Electric',\n    'Mustang Mach-E California Route 1': 'Electric',\n    'Mustang Mach-E GT': 'Electric',\n    'Mustang Mach-E Premium': 'Electric',\n    'Mustang Mach-E Select': 'Electric',\n    'Niro EV EX': 'Electric',\n    'Niro Plug-In Hybrid EX Premium': 'Hybrid',\n    'NV200 SV': 'Gasoline',\n    'Pacifica Launch Edition': 'Hybrid',\n    'Palisade Limited': 'Gasoline',\n    'Panamera Base': 'Gasoline',\n    'Passat 2.0T R-Line': 'Gasoline',\n    'Passat 2.0T SE': 'Gasoline',\n    'Prius v Three': 'Hybrid',\n    'Q3 45 S line Premium': 'Gasoline',\n    'Q3 45 S line Premium Plus': 'Gasoline',\n    'Q4 e-tron 50 Premium Plus': 'Electric',\n    'Q4 e-tron Sportback Premium': 'Electric',\n    'Q5 2.0T Premium Plus': 'Gasoline',\n    'Q5 40 Premium': 'Gasoline',\n    'Q5 S line Premium Plus': 'Gasoline',\n    'Q7 3.0T Prestige': 'Gasoline',\n    'Q7 Premium Plus': 'Gasoline',\n    'Q8 55 Premium': 'Gasoline',\n    'Q8 55 Premium Plus': 'Gasoline',\n    'QX60 Base': 'Gasoline',\n    'R1S Adventure Package': 'Electric',\n    'R1S Launch Edition': 'Electric',\n    'R1T Launch Edition': 'Electric',\n    'R8 5.2': 'Gasoline',\n    'R8 5.2 V10 plus': 'Gasoline',\n    'RAV4 Base': 'Gasoline',\n    'RAV4 TRD Off Road': 'Gasoline',\n    'Revero Base': 'Hybrid',\n    'Rogue SL': 'Gasoline',\n    'Romeo Giulia Quadrifoglio': 'Gasoline',\n    'Rover Range Rover Evoque S': 'Gasoline',\n    'Rover Range Rover P530 SE LWB 7 Seat': 'Gasoline',\n    'Rover Range Rover Sport Supercharged': 'Gasoline',\n    'Rover Range Rover Velar SVAutobiography Dynamic Edition': 'Gasoline',\n    'RS 3 2.5T': 'Gasoline',\n    'S5 3.0T Premium Plus': 'Gasoline',\n    'S5 3.0T Prestige': 'Gasoline',\n    'S90 T5 Momentum': 'Gasoline',\n    'S-Class S 560 4MATIC': 'Gasoline',\n    'S-Class S 63 AMG': 'Gasoline',\n    'Sentra SR': 'Gasoline',\n    'Sierra 1500 Denali': 'Gasoline',\n    'Solstice GXP': 'Gasoline',\n    'Sonata Hybrid Limited': 'Hybrid',\n    'Sorento EX': 'Gasoline',\n    'Tahoe LTZ': 'Gasoline',\n    'Taurus SHO': 'Gasoline',\n    'Taycan': 'Electric',\n    'Taycan 4S': 'Electric',\n    'Taycan Base': 'Electric',\n    'Taycan Turbo': 'Electric',\n    'Titan SV': 'Gasoline',\n    'TLX Type S w/Performance Tire': 'Gasoline',\n    'Transit Connect XLT': 'Gasoline',\n    'Tundra Hybrid TRD Pro': 'Hybrid',\n    'Tundra Limited': 'Gasoline',\n    'Veloster Turbo R-Spec': 'Gasoline',\n    'Wagoneer Series III': 'Gasoline',\n    'Wrangler 80th Anniversary': 'Gasoline',\n    'Wrangler Unlimited Rubicon': 'Gasoline',\n}\n\n\n# 填充 NaN 值\nfor df in [train, test]:\n    df['fuel_type'] = df['fuel_type'].fillna(df['model'].map(fuel_type_mapping))\n\nprint(train.head())\nprint('======================================')\nprint(test.head())\n\n# Feature extraction from 'engine' column\ndef extract_hp(engine):\n    match = re.search(r'(\\d+(\\.\\d+)?)HP', str(engine))\n    return float(match.group(1)) if match else np.nan\n\ndef extract_displacement(engine):\n    match = re.search(r'(\\d+\\.\\d+)L|(\\d+\\.\\d+) Liter', str(engine))\n    return float(match.group(1) or match.group(2)) if match else np.nan\n\ndef extract_engine_type(engine):\n    match = re.search(r'(V\\d+|I\\d+|Flat \\d+|Straight \\d+)', str(engine))\n    return match.group(1) if match else 'Unknown'\n\ndef extract_cylinder_count(engine):\n    match = re.search(r'(\\d+) Cylinder', str(engine))\n    return int(match.group(1)) if match else np.nan\n\ndef extract_fuel_type_engine(engine):\n    fuel_types = ['Gasoline', 'Diesel', 'Electric', 'Hybrid', 'Flex Fuel']\n    for fuel in fuel_types:\n        if fuel in str(engine):\n            return fuel\n    return 'Unknown'\n\nfor df in [train, test]:\n    df['Horsepower'] = df['engine'].apply(extract_hp)\n    df['Displacement'] = df['engine'].apply(extract_displacement)\n    df['Engine_Type'] = df['engine'].apply(extract_engine_type)\n    df['Cylinder_Count'] = df['engine'].apply(extract_cylinder_count)\n    df['Fuel_Type_Engine'] = df['engine'].apply(extract_fuel_type_engine)\n\n# Impute missing values using KNNImputer\nimputer = KNNImputer(n_neighbors=5)\n\n# Combine train and test data for consistent imputation\ncombined = pd.concat([train, test], sort=False)\n\n# Impute 'Horsepower' and 'Cylinder_Count'\ncombined[['Horsepower', 'Cylinder_Count']] = imputer.fit_transform(combined[['Horsepower', 'Cylinder_Count']])\n\n# Split back to train and test\ntrain[['Horsepower', 'Cylinder_Count']] = combined.loc[combined['price'].notnull(), ['Horsepower', 'Cylinder_Count']]\ntest[['Horsepower', 'Cylinder_Count']] = combined.loc[combined['price'].isnull(), ['Horsepower', 'Cylinder_Count']]\n\n# Drop 'engine' and 'model' columns\ntrain.drop(columns=['engine', 'model'], inplace=True, errors='ignore')\ntest.drop(columns=['engine', 'model'], inplace=True, errors='ignore')\n\n# Clear memory\ndel combined\ngc.collect()\n\n# Encode 'accident' and 'clean_title'\ntrain[\"accident\"] = train[\"accident\"].replace({'At least 1 accident or damage reported':1,\"None reported\":0}) \ntest[\"accident\"] = test[\"accident\"].replace({'At least 1 accident or damage reported':1,\"None reported\":0}) \n\ntrain[\"clean_title\"] = train[\"clean_title\"].replace({\"Yes\":1,\"No\":0})\ntest[\"clean_title\"] = test[\"clean_title\"].replace({\"Yes\":1,\"No\":0})\n\n# Process 'transmission' column\ntransmission_mapping = {\n    '6-speed a/t': 'Automatic',\n    '8-speed automatic': 'Automatic',\n    'automatic': 'Automatic',\n    '7-speed a/t': 'Automatic',\n    'a/t': 'Automatic',\n    '8-speed a/t': 'Automatic',\n    'transmission w/dual shift mode': 'Automatic',\n    '9-speed automatic': 'Automatic',\n    '10-speed automatic': 'Automatic',\n    '1-speed a/t': 'Automatic',\n    '2-speed a/t': 'Automatic',\n    '2-speed automatic': 'Automatic',\n    '4-speed a/t': 'Automatic',\n    '5-speed automatic': 'Automatic',\n    '4-speed automatic': 'Automatic',\n    '6-speed automatic': 'Automatic', \n    '9-speed a/t': 'Automatic',        \n    '10-speed a/t': 'Automatic',      \n    '7-speed automatic': 'Automatic',  \n    '6-speed electronically controlled automatic with o': 'Automatic',\n    'single-speed fixed gear': 'Automatic',\n    '7-speed dct automatic': 'Automatic',\n    '10-speed automatic with overdrive': 'Automatic',\n    'automatic, 9-spd 9g-tronic': 'Automatic',\n    'automatic, 8-spd': 'Automatic',\n    'automatic, 8-spd sport w/sport & manual modes': 'Automatic',\n    'automatic, 8-spd pdk dual-clutch': 'Automatic',\n    'automatic, 8-spd m steptronic w/drivelogic, sport & manual modes': 'Automatic',\n    'automatic, 8-spd dual-clutch': 'Automatic',\n    'transmission overdrive switch': 'Automatic',  \n\n    '7-speed automatic with auto-shift': 'Tiptronic',\n    '5-speed a/t': 'Tiptronic',\n    '7-speed a/t tiptronic': 'Tiptronic',  \n    '8-speed at': 'Tiptronic',\n    '8-speed a/t': 'Tiptronic',\n\n    '6-speed m/t': 'Manual',\n    '7-speed m/t': 'Manual',\n    '6-speed manual': 'Manual',\n    '5-speed m/t': 'Manual',\n    'manual': 'Manual',\n    '7-speed manual': 'Manual',\n    '8-speed manual': 'Manual',\n    'm/t': 'Manual',\n    '6 speed at/mt': 'Manual',\n    '6 speed mt': 'Manual',\n\n    'automatic cvt': 'Variator',\n    'cvt transmission': 'Variator',\n    'cvt-f': 'Variator',\n\n    'variable': 'Variator',\n    'f': 'Other',                        \n    '7-speed': 'Other',                 \n    '6-speed': 'Other',                  \n    '2': 'Other',                       \n    '–': 'Other',\n    'scheduled for or in production': 'Other'\n}\n\ndef simplify_transmission(transmission):\n    if 'Automatic' in transmission:\n        return 'Automatic'\n    elif 'Manual' in transmission:\n        return 'Manual'\n    elif 'Tiptronic' in transmission:\n        return 'Tiptronic'\n    elif 'Variator' in transmission:\n        return 'Variator'\n    else:\n        return 'Other'\n\nfor df in [train, test]:\n    df['transmission'] = df['transmission'].str.strip().str.lower()\n    df['transmission'] = df['transmission'].replace(transmission_mapping)\n    df['transmission'] = df['transmission'].apply(simplify_transmission)\n\n# Encode 'transmission' numerically\ntransmission_encoding = {'Automatic':1,\n                         'Tiptronic':2,\n                         'Manual':3,\n                         'Variator':4,\n                         'Other':5}\ntrain[\"transmission\"] = train[\"transmission\"].map(transmission_encoding)\ntest[\"transmission\"] = test[\"transmission\"].map(transmission_encoding)\n\n# Drop 'Fuel Type' column\ntrain.drop('Fuel Type', axis=1, inplace=True, errors='ignore')\ntest.drop('Fuel Type', axis=1, inplace=True, errors='ignore')\n\n# Impute missing values in 'Displacement'\ntrain['Displacement'].fillna(value=train['Displacement'].mean(), inplace=True)\ntest['Displacement'].fillna(value=test['Displacement'].mean(), inplace=True)\n\n# Encode categorical variables\ncategorical_columns = ['brand', 'fuel_type', 'ext_col', 'int_col', 'Engine_Type', 'Fuel_Type_Engine']\n\nfor col in categorical_columns:\n    le = LabelEncoder()\n    combined_col = pd.concat([train[col], test[col]], axis=0)\n    le.fit(combined_col.astype(str))\n    train[col] = le.transform(train[col].astype(str))\n    test[col] = le.transform(test[col].astype(str))\n\n# Scale numerical features\nnumerical_features = ['Horsepower', 'Displacement', 'Cylinder_Count', 'milage', 'model_year']\n\nscaler = StandardScaler()\ntrain[numerical_features] = scaler.fit_transform(train[numerical_features])\ntest[numerical_features] = scaler.transform(test[numerical_features])\n\n# Clear memory\ngc.collect()\n\n# Prepare data for modeling\nX = train.drop(columns=['id', 'price'])\ny = train['price']\nX_test = test.drop(columns=['id'])\n\n# Set up cross-validation\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T23:19:49.981274Z","iopub.execute_input":"2024-09-23T23:19:49.981860Z","iopub.status.idle":"2024-09-23T23:19:50.653184Z","shell.execute_reply.started":"2024-09-23T23:19:49.981793Z","shell.execute_reply":"2024-09-23T23:19:50.651726Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('===========================================================================')\nX.head()\nprint('===========================================================================')\ny.head()\nprint('===========================================================================')\nX_test.head()\nprint('===========================================================================')\nprint(folds)\nprint('===========================================================================')","metadata":{"execution":{"iopub.status.busy":"2024-09-23T23:19:57.974752Z","iopub.execute_input":"2024-09-23T23:19:57.975233Z","iopub.status.idle":"2024-09-23T23:19:57.984384Z","shell.execute_reply.started":"2024-09-23T23:19:57.975151Z","shell.execute_reply":"2024-09-23T23:19:57.982989Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to optimize and train models with Optuna\ndef optimize_model(model_name, X, y):\n    def objective(trial):\n        if model_name == 'catboost':\n            param = {\n                'iterations': trial.suggest_int('iterations', 1000, 3000),\n                'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n                'depth': trial.suggest_int('depth', 4, 10),\n                'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-2, 10),\n                'random_strength': trial.suggest_uniform('random_strength', 0, 10),\n                'bagging_temperature': trial.suggest_uniform('bagging_temperature', 0, 10),\n                'eval_metric': 'RMSE',\n                'loss_function': 'RMSE',\n                'verbose': False,\n                'task_type': 'CPU',\n                'random_seed': 42,\n            }\n            oof_preds = np.zeros(len(X))\n            for fold, (train_idx, val_idx) in enumerate(folds.split(X, y)):\n                X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n                X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n                model = CatBoostRegressor(**param)\n                model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=100, verbose=False)\n                oof_preds[val_idx] = model.predict(X_val)\n            rmse = np.sqrt(mean_squared_error(y, oof_preds))\n            return rmse\n        elif model_name == 'lightgbm':\n            param = {\n                'objective': 'regression',\n                'metric': 'rmse',\n                'verbosity': -1,\n                'boosting_type': 'gbdt',\n                'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),\n                'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n                'num_leaves': trial.suggest_int('num_leaves', 31, 256),\n                'max_depth': trial.suggest_int('max_depth', 4, 12),\n                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n                'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n                'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n                'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10),\n                'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 10),\n                'random_state': 42\n            }\n            oof_preds = np.zeros(len(X))\n            for fold, (train_idx, val_idx) in enumerate(folds.split(X, y)):\n                X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n                X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n                model = LGBMRegressor(**param)\n                model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n                oof_preds[val_idx] = model.predict(X_val)\n            rmse = np.sqrt(mean_squared_error(y, oof_preds))\n            return rmse\n        elif model_name == 'xgboost':\n            param = {\n                'objective': 'reg:squarederror',\n                'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),\n                'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n                'max_depth': trial.suggest_int('max_depth', 4, 12),\n                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n                'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n                'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n                'gamma': trial.suggest_loguniform('gamma', 1e-4, 10),\n                'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10),\n                'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 10),\n                'random_state': 42,\n                'tree_method': 'hist',\n                'predictor': 'cpu_predictor'\n            }\n            oof_preds = np.zeros(len(X))\n            for fold, (train_idx, val_idx) in enumerate(folds.split(X, y)):\n                X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n                X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n                model = XGBRegressor(**param)\n                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n                oof_preds[val_idx] = model.predict(X_val)\n            rmse = np.sqrt(mean_squared_error(y, oof_preds))\n            return rmse\n    study = optuna.create_study(direction='minimize')\n    study.optimize(objective, n_trials=10)\n    return study.best_params\n\n# Optimize models\nbest_params_cat = optimize_model('catboost', X, y)\nbest_params_lgb = optimize_model('lightgbm', X, y)\nbest_params_xgb = optimize_model('xgboost', X, y)\n\n# Optimize Neural Network with Optuna\ndef optimize_nn(X, y):\n    def objective(trial):\n        # Clear session\n        K.clear_session()\n        # Hyperparameters to tune\n        num_layers = trial.suggest_int('num_layers', 2, 5)\n        units = trial.suggest_int('units', 128, 512)\n        activation = trial.suggest_categorical('activation', ['relu', 'elu', 'selu'])\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n        batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n        dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.2)\n        # Build the model\n        inputs = Input(shape=(X.shape[1],))\n        x = inputs\n        for i in range(num_layers):\n            x = Dense(units, activation=activation)(x)\n            x = BatchNormalization()(x)\n            x = Dropout(dropout_rate)(x)\n        outputs = Dense(1)(x)\n        model = Model(inputs, outputs)\n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mean_squared_error')\n        oof_preds = np.zeros(len(X))\n        for fold, (train_idx, val_idx) in enumerate(folds.split(X, y)):\n            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n            X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n            # Convert to numpy arrays\n            X_train_np = X_train.values.astype(np.float32)\n            X_val_np = X_val.values.astype(np.float32)\n            model.fit(X_train_np, y_train, epochs=100, batch_size=batch_size, verbose=0,\n                      validation_data=(X_val_np, y_val), callbacks=[EarlyStopping(patience=10, restore_best_weights=True)])\n            oof_preds[val_idx] = model.predict(X_val_np).flatten()\n        rmse = np.sqrt(mean_squared_error(y, oof_preds))\n        return rmse\n    study = optuna.create_study(direction='minimize')\n    study.optimize(objective, n_trials=2)\n    return study.best_params\n\nbest_params_nn = optimize_nn(X, y)\n\n# Retrain models with best parameters and get out-of-fold predictions\ndef train_and_predict(model_name, X, y, X_test, params):\n    oof_preds = np.zeros(len(X))\n    test_preds = np.zeros(len(X_test))\n    for fold, (train_idx, val_idx) in enumerate(folds.split(X, y)):\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n        if model_name == 'catboost':\n            model = CatBoostRegressor(**params)\n            model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=100, verbose=False)\n        elif model_name == 'lightgbm':\n            model = LGBMRegressor(**params)\n            model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n        elif model_name == 'xgboost':\n            model = XGBRegressor(**params)\n            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n        oof_preds[val_idx] = model.predict(X_val)\n        test_preds += model.predict(X_test) / folds.n_splits\n    rmse = np.sqrt(mean_squared_error(y, oof_preds))\n    print(f\"{model_name} CV RMSE: {rmse}\")\n    return oof_preds, test_preds\n\noof_preds_cat, test_preds_cat = train_and_predict('catboost', X, y, X_test, best_params_cat)\noof_preds_lgb, test_preds_lgb = train_and_predict('lightgbm', X, y, X_test, best_params_lgb)\noof_preds_xgb, test_preds_xgb = train_and_predict('xgboost', X, y, X_test, best_params_xgb)\n\n# Neural Network training\ndef build_nn_model(params):\n    inputs = Input(shape=(X.shape[1],))\n    x = inputs\n    for i in range(params['num_layers']):\n        x = Dense(params['units'], activation=params['activation'])(x)\n        x = BatchNormalization()(x)\n        x = Dropout(params['dropout_rate'])(x)\n    outputs = Dense(1)(x)\n    model = Model(inputs, outputs)\n    model.compile(optimizer=tf.keras.optimizers.Adam(params['learning_rate']), loss='mean_squared_error')\n    return model\n\noof_preds_nn = np.zeros(len(X))\ntest_preds_nn = np.zeros(len(X_test))\nfor fold, (train_idx, val_idx) in enumerate(folds.split(X, y)):\n    K.clear_session()\n    model = build_nn_model(best_params_nn)\n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n    X_train_np = X_train.values.astype(np.float32)\n    X_val_np = X_val.values.astype(np.float32)\n    X_test_np = X_test.values.astype(np.float32)\n    model.fit(X_train_np, y_train, epochs=100, batch_size=best_params_nn['batch_size'], verbose=0,\n              validation_data=(X_val_np, y_val), callbacks=[EarlyStopping(patience=10, restore_best_weights=True)])\n    oof_preds_nn[val_idx] = model.predict(X_val_np).flatten()\n    test_preds_nn += model.predict(X_test_np).flatten() / folds.n_splits\nrmse_nn = np.sqrt(mean_squared_error(y, oof_preds_nn))\nprint(f\"Neural Network CV RMSE: {rmse_nn}\")\n\n# Create meta-features for stacking\nX_meta = pd.DataFrame({\n    'catboost': oof_preds_cat,\n    'lightgbm': oof_preds_lgb,\n    'xgboost': oof_preds_xgb,\n    'nn': oof_preds_nn,\n})\nX_test_meta = pd.DataFrame({\n    'catboost': test_preds_cat,\n    'lightgbm': test_preds_lgb,\n    'xgboost': test_preds_xgb,\n    'nn': test_preds_nn,\n})\n\n# Optimize meta-model with Optuna\ndef optimize_meta(X_meta, y):\n    def objective(trial):\n        alpha = trial.suggest_loguniform('alpha', 1e-4, 10)\n        oof_meta = np.zeros(len(y))\n        for fold, (train_idx, val_idx) in enumerate(folds.split(X_meta, y)):\n            X_train_meta, y_train_meta = X_meta.iloc[train_idx], y.iloc[train_idx]\n            X_val_meta, y_val_meta = X_meta.iloc[val_idx], y.iloc[val_idx]\n            model_meta = Ridge(alpha=alpha)\n            model_meta.fit(X_train_meta, y_train_meta)\n            oof_meta[val_idx] = model_meta.predict(X_val_meta)\n        rmse = np.sqrt(mean_squared_error(y, oof_meta))\n        return rmse\n    study = optuna.create_study(direction='minimize')\n    study.optimize(objective, n_trials=2)\n    return study.best_params\n\nbest_params_meta = optimize_meta(X_meta, y)\nbest_alpha = best_params_meta['alpha']\n\n# Train final meta-model\nmodel_meta = Ridge(alpha=best_alpha)\nmodel_meta.fit(X_meta, y)\nfinal_predictions = model_meta.predict(X_test_meta)\noof_meta = model_meta.predict(X_meta)\nrmse_meta = np.sqrt(mean_squared_error(y, oof_meta))\nprint(f\"Meta-model CV RMSE: {rmse_meta}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-23T23:20:55.619313Z","iopub.execute_input":"2024-09-23T23:20:55.619787Z","iopub.status.idle":"2024-09-23T23:22:44.488059Z","shell.execute_reply.started":"2024-09-23T23:20:55.619743Z","shell.execute_reply":"2024-09-23T23:22:44.486597Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create submission file\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'price': final_predictions\n})\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created: submission.csv\")\nsubmission.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-23T23:22:53.312699Z","iopub.execute_input":"2024-09-23T23:22:53.313933Z","iopub.status.idle":"2024-09-23T23:22:53.338979Z","shell.execute_reply.started":"2024-09-23T23:22:53.313873Z","shell.execute_reply":"2024-09-23T23:22:53.335711Z"},"trusted":true},"outputs":[],"execution_count":null}]}